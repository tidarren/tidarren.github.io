---
title: "MM-LLMs: Recent Advances in MultiModal Large Language Models"
date: 2024-05-12 20:30:00 +0800
categories: [Paper, MMLLM]
tags: [mmllm, mllm, llm, vlm]     # TAG names should always be lowercase
math: true
image:
  path: assets/img/20240512/general_model_architecture.png
  alt: General model architecture
---

Paper link: [https://arxiv.org/abs/2401.13601](https://arxiv.org/abs/2401.13601)

## Goal
æ•´ç†äº† MultiModal Large Language Modelï¼ˆMMLLMï¼‰çš„ç™¼å±•å’Œç ”ç©¶ç¾æ³ï¼ŒåŒ…å«æ¨¡å‹è¨“ç·´æ¶æ§‹åŠç¯„å¼ã€State-of-the-artï¼ˆSOTAï¼‰çš„ MMLLM æœ‰å“ªäº›ã€å…¶æ¶æ§‹çš„çµ„åˆç‚ºä½•ç­‰ç­‰ï¼Œä¸¦é»å‡ºæœªä¾†æ½›åœ¨çš„ç ”ç©¶æ–¹å‘ã€‚

## Training Architecture
### Overview
æ¨¡å‹è¨“ç·´æ¶æ§‹å¯åƒè€ƒä»¥ä¸‹é€™å¼µåœ–ï¼š
![](assets/img/20240512/general_model_architecture.png)
_General model architecture_

å¦‚åœ–æ‰€ç¤ºï¼ŒåŸºæœ¬ä¸Šå¯åˆ†æˆå…©å¤§éƒ¨åˆ†ï¼š
- Multimodal Understanding
- Multimodal Generation

ä¸¦ç”±5å€‹ component çµ„æˆï¼š
- Modality Encoderï¼ˆ$ME_x$ï¼‰
- Input Projectorï¼ˆ$\Theta_{X \to T}$ï¼‰
- LLM Backbone
- Output Projectorï¼ˆ$\Theta_{T \to X}$ï¼‰
- Modality Decoderï¼ˆ$MG_x$ï¼‰

è‹¥ä»¥ $X \to X$ è¡¨ç¤ºæ¨¡å‹çš„ I/Oï¼ˆXä»£è¡¨ä»»æ„æ¨¡æ…‹ï¼Œå¯èƒ½æ˜¯ I: imageã€V: videoã€A: audio æˆ–å°±æ˜¯ T: textï¼‰ï¼Œ Multimodal Understanding çš„ç¯„ç–‡å³å¯è¡¨ç¤ºæˆï¼š$X \to T$ã€‚

### Bridge between Embedding and LLMï¼šInput Projector
ä¸€èˆ¬è€Œè¨€ï¼Œç‚ºé¿å…å½±éŸ¿ LLM çš„æ—¢æœ‰èƒ½åŠ›ï¼Œæˆ–è€…èªªé€ æˆ Catastrophic Forgettingï¼Œåœ¨è¨“ç·´æ™‚æœƒå°‡ LLM çš„åƒæ•¸å‡çµï¼ˆå¯çœ‹åˆ° LLM Backbone å³ä¸‹è§’ä»¥ â„ï¸ è¡¨ç¤ºï¼‰ï¼Œè€Œé€™æ™‚ Input Projector å°±è² è²¬æ‰®æ¼”å°‡ Modality Encoder çš„çµæœï¼ˆ $F_X$ ï¼‰å»é©é… LLM è¼¸å…¥çš„é‡è¦è§’è‰²ã€‚è² è²¬è½‰æ›çš„é€™éç¨‹æ˜¯éœ€è¦è¨“ç·´çš„ï¼Œæ•…å¯çœ‹åˆ°åœ–ä¸­ Input Projector çš„å³ä¸‹è§’æœ‰ğŸ”¥ï¼Œè¡¨ç¤ºé€™éƒ¨åˆ†çš„æ¨¡å‹åƒæ•¸æ˜¯ learnableã€‚

### Input Projectorï¼šCross-attention-based vs Auto-regressive-based
Input Projector å¯åˆ†æˆå…©ç¨® settingï¼š
- Cross-attention-based
- Auto-regressive-based

Cross-attention-based çš„ä»£è¡¨ MMLLM æ˜¯ [BLIP-2](https://arxiv.org/abs/2301.12597)ï¼Œå°‡ $F_X$ é€é Q-Formerï¼ˆå…·é«”è€Œè¨€æ˜¯ $BERT_{base}$ï¼‰å’Œ text representation å»äº¤äº’ä½œç”¨ã€‚
![BLIP-2](assets/img/20240512/q-former.png)
_BLIP-2_

Auto-regressive-based çš„ä»£è¡¨ MMLLM å‰‡æ˜¯ [VILA](https://arxiv.org/abs/2312.07533)ï¼Œé€™å€‹æ–¹æ³•è¼ƒç‚ºç›´è¦ºç†è§£ï¼Œå³æ˜¯æŠŠ Input Projector è¦–ä½œä¸€ç¨® tokenizerï¼Œå°‡ $F_X$ å†è½‰æˆ LLM å¯æ¥å—çš„ token sequenceã€‚å¦‚ä¸‹åœ–è¡¨ç¤ºï¼Œvisual tokens å’Œ textual tokens å¯å½ˆæ€§æ”¾ç½®ä»¥æ”¯æ´ä»»æ„çš„ interleaved
image-text è¼¸å…¥ã€‚

<a name="VILA"></a>

![VILA](assets/img/20240512/vila.png)
_VILA_


## Training Paradigm
å’Œ LLM çš„è¨“ç·´ç¯„å¼ç›¸åŒï¼Œä¹Ÿæœ‰ Pre-Trainingï¼ˆPTï¼‰å’Œ Instruction Tuningï¼ˆITï¼‰å…©å¤§éšæ®µã€‚

### Pre-Trainingï¼ˆPTï¼‰
PT æ‰€ä½¿ç”¨çš„è¨“ç·´è³‡æ–™åŸºæœ¬ä¸Šæ˜¯ X-Text pair çš„å½¢å¼ï¼Œå…¶ä¸­ Image-Text é™¤äº† pair ä¹‹å¤–ï¼ˆ`<img1><txt1>`ï¼‰ï¼Œé‚„æœ‰åƒæ˜¯åœ¨ä»‹ç´¹ VILA æ™‚æåˆ°çš„ interleaved image-text æ ¼å¼ï¼š`<txt1><img1><txt2><txt3><img2><txt4>`ã€‚

### Instruction Tuningï¼ˆITï¼‰
IT ä¸€æ¨£æœ‰å…©å€‹æ­¥é©Ÿï¼ŒSupervised Fine-Tuningï¼ˆSFTï¼‰å’Œ Reinforcement Learning from Human Feedbackï¼ˆRLHFï¼‰ã€‚SFT å°‡éƒ¨åˆ†çš„ PT è¨“ç·´è³‡æ–™è½‰ç‚º instruction-aware çš„æ ¼å¼ï¼Œä»¥ Visual Quetsion- Answeringï¼ˆVQAï¼‰ç‚ºä¾‹ï¼Œæ¨¡æ¿å¯èƒ½ç‚ºï¼š
- `<Image>{Question} A short answer to the question`
- `<Image> Examine the image and respond to the following question with a brief answer: {Question}. Answer:`

RLHF æ¥è‘— SFT å¾Œï¼Œæ ¹æ“šæ¨¡å‹çš„è¼¸å‡ºï¼Œè—‰ç”± Natural Language Feedbackï¼ˆNLFï¼‰æ›´é€²ä¸€æ­¥å¾®èª¿ã€‚Framework å¯åƒè€ƒï¼š[RL4F](https://arxiv.org/abs/2305.08844)ã€[LLaVA-RLHF](https://arxiv.org/abs/2309.14525) å’Œ [DRESS](https://arxiv.org/abs/2311.10081)ã€‚

### Training recipe without bells and whistles
å¾ [VILA åœ–çš„å³åŠé‚Š](#VILA) å¯ä»¥çœ‹åˆ°ï¼ŒLLM åœ¨ PT å’Œ ITçš„éšæ®µï¼Œä¸¦é frozen çš„ç‹€æ…‹ï¼Œä¸»è¦æ˜¯èªç‚ºï¼Œé›–ç„¶ frozen LLM åœ¨ zero-shot çš„è¡¨ç¾ä¸éŒ¯ï¼Œä½†æ¶‰åŠåˆ° few-shotï¼Œ ä¹Ÿå°±æ˜¯ in-context learningï¼ˆICLï¼‰çš„ä»»å‹™æ™‚å°±æœ‰å·®äº†ã€‚ ä¹Ÿè«‡åˆ° interleaved image-text æ¯”èµ· image-text pair æ›´èƒ½å¹«åŠ©æ¨¡å‹ï¼Œä¸»è¦æ˜¯å¾Œè€…çš„ caption å¤§å¤šè¼ƒç‚ºç°¡çŸ­ï¼Œå’ŒåŸå…ˆ LLM æ‰€è¨“ç·´çš„ text-only corpus çš„ distribution æœ‰æ‰€è½å·®ï¼Œæœƒé€ æˆæ¨¡å‹ Catastrophic Forgettingï¼Œç›¸æ¯”ä¹‹ä¸‹ interleaved image-text é€šå¸¸ text çš„è³‡è¨Šè¼ƒç‚ºè±å¯Œã€‚è€Œå¯¦é©—è­‰æ˜å…©è€…èåˆæ›´èƒ½å¢åŠ è³‡æ–™çš„è±å¯Œåº¦ï¼Œé”åˆ°æ›´å¥½çš„è¡¨ç¾ã€‚PT éšæ®µå¾Œï¼ŒJoint SFT æåˆ°é™¤äº†ä½¿ç”¨å«æœ‰ Image-Text çš„ data fine-tuning å¤–ï¼Œtext-only corpus ä¹Ÿæ‡‰åŠ é€²å»ä¸€èµ· SFTï¼Œä¸åƒ…å¯æœ‰æ•ˆé¿å… LLM åœ¨ text-only çš„ä»»å‹™ä¸Šè¡¨ç¾é™ä½ï¼Œä¹Ÿèƒ½å”åŠ©å…¶æ‰¾å› instruction following èƒ½åŠ›ï¼Œæå‡åœ¨ visual task çš„è¡¨ç¾ã€‚

## SOTA MMLLM
### Component Anatomy
MMLLM ä»¥åŠå…¶ component ä¸€è¦½å¯åƒè€ƒä¸‹è¡¨ï¼š
![](assets/img/20240512/sota-mllm.png)

### Trends
- è¨“ç·´ç¯„å¼çš„é€²å±•ï¼šPT $\to$ SFT $\to$ RLHFï¼Œæ›´åŠ å¢å¼·æ¨¡å‹çš„å°è©±èƒ½åŠ›
    - [BLIP-2](https://arxiv.org/abs/2301.12597) $\to$ [InsturctBLIP](https://arxiv.org/abs/2305.06500) $\to$ [DRESS](https://arxiv.org/abs/2311.10081)
- æ›´é«˜å“è³ªçš„è¨“ç·´è³‡æ–™
    - [LLaVa](https://arxiv.org/abs/2304.08485) $\to$ [LLaVa-1.5](https://arxiv.org/abs/2310.03744)
- æ›´æœ‰æ•ˆç‡çš„è¨“ç·´æ–¹å¼ï¼Œå°‡ Input Projector åŒ–ç¹ç‚ºç°¡ï¼Œå¾è¤‡é›œçš„ Q-former åˆ°ç°¡å–®çš„ Linear Projector
    - [BLIP-2](https://arxiv.org/abs/2301.12597) $\to$ [VILA](https://arxiv.org/abs/2312.07533) 


## Future Directions

### More Challenging Benchmarks
[MMMU](https://huggingface.co/datasets/MMMU/MMMU) å’Œ [CMMMU](https://huggingface.co/datasets/m-a-p/CMMMU) ä»£è¡¨äº† Massive Multi-discipline Multimodal Understanding çš„è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬ï¼Œç”±å¤§å­¸ç¨‹åº¦çš„å…­å¤§å­¸ç§‘å¤šé¸é¡Œçµ„æˆï¼ŒåŒ…å«ï¼šè—è¡“è¨­è¨ˆã€å•†æ¥­ã€ç§‘å­¸ã€å¥åº·é†«å­¸ã€äººæ–‡ç¤¾æœƒå’Œç§‘æŠ€å·¥ç¨‹ï¼Œæ¶µè“‹30ç¨®ä¸»é¡Œï¼Œä»¥åŠèµ·ç¢¼30ç¨®çš„åœ–ç‰‡é¡å‹ï¼Œä¾‹å¦‚ï¼šåœ–è¡¨ã€æµç¨‹åœ–ã€åœ°åœ–ã€è¡¨æ ¼ã€æ¨‚è­œç”šè‡³æ˜¯åŒ–å­¸çµæ§‹å¼ã€‚å¯çœ‹æˆæ˜¯ vision ç‰ˆæœ¬çš„ [TMMLU+](https://huggingface.co/datasets/ikala/tmmluplus)ã€‚

![](https://cdn-uploads.huggingface.co/production/uploads/6230d750d93e84e233882dbc/2Ulh9yznm1dvISV4xJ_Ok.png)
_MMMU_

### Mobile/Lightweight Deployment

æ¨¡å‹æœ¬èº«å¦‚ä½•è¼•é‡åŒ–å·²æ˜¯ä¸€å€‹é‡è¦çš„è¶¨å‹¢ï¼Œå¦‚ä½•åœ¨åƒæ˜¯è¡Œå‹•æˆ–ç‰©è¯ç¶²ç­‰è¨ˆç®—è³‡æºæœ‰é™çš„è£ç½®ä¸Šéƒ¨ç½²ï¼Œåœ¨æ¸›å°æ¨¡å‹è¦æ¨¡çš„æ¢ä»¶ä¸‹åŒæ™‚é‚„èƒ½ä¿æœ‰ä¸€å®šæ€§èƒ½ï¼Œæ˜¯æ›´ç¬¦åˆå•†æ¥­éœ€æ±‚çš„èª²é¡Œã€‚[MoE-LLaVA](https://arxiv.org/abs/2401.15947) å°‡ MoE-Tuning å¼•å…¥åˆ° MMLLM çš„ç¯„ç–‡ç•¶ä¸­ï¼Œä»¥å¤§ç´„ 3B çš„ activated åƒæ•¸é‡ï¼Œå¯é”åˆ°è·Ÿ 7B å¤§å°çš„ SOTA æ¨¡å‹å·®ä¸å¤šçš„è¡¨ç¾ã€‚ [MobileVLMV2](https://arxiv.org/abs/2402.03766) å‰‡æ˜¯é€²ä¸€æ­¥å¾ Input Projector è‘—æ‰‹æ”¹è‰¯ï¼Œæ‰€æå‡ºçš„ LDPv2ï¼Œå…¶ä¸­çš„ positonal part ç›¸æ¯”å‰ä»£æ¸›å°‘99.8%çš„åƒæ•¸é‡ï¼Œå¤§å¹…å¢åŠ æ¨è«–é€Ÿåº¦ã€‚

![](assets/img/20240512/mobileVLM.png){: width="500" height="200" }
_MobileVLMV2_
